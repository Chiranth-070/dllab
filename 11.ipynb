import torch, torch.nn as nn

class MHA(nn.Module):
    def __init__(self, dim, heads):
        super().__init__()
        assert dim % heads == 0
        self.heads, self.head_dim = heads, dim // heads
        self.qkv = nn.Linear(dim, 3 * dim)
        self.out = nn.Linear(dim, dim)

    def forward(self, x, mask=None):
        B, S, _ = x.size()
        q, k, v = self.qkv(x).chunk(3, -1)
        def reshape(t): return t.view(B, S, self.heads, self.head_dim).transpose(1, 2)
        q, k, v = map(reshape, (q, k, v))
        scores = (q @ k.transpose(-2, -1)) / self.head_dim**0.5
        if mask is not None: scores = scores.masked_fill(mask == 0, -1e9)
        attn = (scores.softmax(-1) @ v).transpose(1, 2).reshape(B, S, -1)
        return self.out(attn)

# Test
mha = MHA(128, 8)
x = torch.rand(2, 10, 128)
print(mha(x).shape)  # torch.Size([2, 10, 128])