import numpy as np
import matplotlib.pyplot as plt

def softmax(x):
    return np.exp(x-np.max(x))/ np.sum(np.exp(x-np.max(x)))

def ccel(y_pred , y_true):
    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)
    return -np.sum(y_true * np.log(y_pred))/ y_true.shape[0]

def grad(y_pred, y_true):
    return y_pred - y_true

def adjusted_lr(init_lr, epoch):
    return init_lr / (1 + 0.1 *(epoch//10))

def fwd (X,w):
    return softmax(X @ w)

def bwd (X,y_true, y_pred):
    return (X.T @ grad(y_pred,y_true))/X.shape[0]

X=np.array([[1,2],[1.5,2.5],[2,3]])
w= np.array([[0.2,-0.3,0.5],[-0.5,0.7,-0.2]])
y= np.array([[0,1,0],[1,0,0],[0,0,1]])


y_pred= fwd(X,w)


print("\nPredicted:", y_pred,
      "\nLoss:",ccel(y_pred, y),
      "\n gradient:", bwd(X,y_pred, y),
      "\n Adjusted LR:", adjusted_lr(0.01,20))

s= np.linspace(-2,2,5)
plt.plot(s,softmax(s), marker="o")
plt.title("Softmax Output")
plt.show()

print("Softmax: ", softmax(s))